{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd6a2e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "867a11cd",
   "metadata": {},
   "source": [
    "# Keras Basics with Classifying Movie Reviews Examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33893395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 3.11.3\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d739712",
   "metadata": {},
   "source": [
    "### 1. Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8a0dbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported imdb\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from keras.datasets import imdb\n",
    "    print(\"Successfully imported imdb\")\n",
    "except ImportError:\n",
    "    print(\"Error importing imdb\")\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd8fe37",
   "metadata": {},
   "source": [
    "**The `imdb.load_data()` function** is designed to always return **one package** containing all four items at once:\n",
    "- `train_data`\n",
    "- `train_labels`\n",
    "- `test_data`\n",
    "- `test_labels`\n",
    "\n",
    "**`num_words=10000`**\n",
    "This is a **filter**.  \n",
    "It tells the function: *\"Only give me the numbers for the top 10,000 most frequent words.\"*\n",
    "\n",
    "Any word that isn't in that top 10,000 (like a **rare name** or a **typo**) is marked as an **unknown word** and given a **special number**.  \n",
    "This keeps the **vocabulary size manageable**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad55ddfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: <class 'numpy.ndarray'> (25000,)\n",
      "train_labels: <class 'numpy.ndarray'> (25000,)\n",
      "test_data: <class 'numpy.ndarray'> (25000,)\n",
      "test_labels: <class 'numpy.ndarray'> (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_data:\", type(train_data), train_data.shape)\n",
    "print(\"train_labels:\", type(train_labels), train_labels.shape)\n",
    "print(\"test_data:\", type(test_data), test_data.shape)\n",
    "print(\"test_labels:\", type(test_labels), test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0618d19b",
   "metadata": {},
   "source": [
    "The variables `train_data` and `test_data` are lists of reviews, each review being a list of word indices (encoding a sequence of words). \n",
    "`train_labels` and `test_labels` are lists of **0s** and **1s**, where 0 stands for \"negative\" and 1 stands for \"positive\".\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2748f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 189\n",
      "<class 'list'> 141\n",
      "<class 'list'> 550\n",
      "<class 'list'> 158\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data[1]), len(train_data[1]))\n",
    "print(type(train_data[2]), len(train_data[2]))  \n",
    "print(type(train_data[3]), len(train_data[3]))\n",
    "print(type(train_data[24001]), len(train_data[24001]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22b93fe",
   "metadata": {},
   "source": [
    "The purpose of these lines is to **investigate the data**.  \n",
    "Before we try to build a model, it's a good practice to **look at our data closely** to understand its structure.\n",
    "\n",
    "Let's break down one line:  \n",
    "```\n",
    "print(type(train_data[1]), len(train_data[1]))\n",
    "```\n",
    "\n",
    "- **`train_data[1]`**: This selects a single review from our dataset â€” the second review, in this case *(since Python indexing starts at 0)*.\n",
    "\n",
    "- **`type(...)`**: This tells us the data type of that review.  \n",
    "  The output is:\n",
    "  ```\n",
    "  <class 'list'>\n",
    "  ```\n",
    "  This confirms each review is a list of numbers.\n",
    "\n",
    "- **`len(...)`**: This tells us the length of that list, which is the number of words in that specific review.\n",
    "\n",
    "The output shows lengths like:\n",
    "```\n",
    "189, 141, 550\n",
    "```\n",
    "\n",
    "\n",
    "So, the whole reason for running this code is to **prove two things**:\n",
    "1. Each review is a simple list.  \n",
    "2. More importantly, the reviews all have different lengths.\n",
    "\n",
    "This confirms the problem we discussed:  \n",
    "The data is **not uniform**, which is why we will need to **vectorize it**.\n",
    "\n",
    "`Vectorizing` means converting movie reviews, which are currently lists of word IDs with different lengths, into a fixed-size numeric format that a machine learning model can understand. Since neural networks require inputs of the same shape, we must standardize the data. This can be done by one-hot encoding, where each review is turned into a long vector of 0s and 1s representing the presence of specific words, or by padding sequences, where all reviews are cut or padded with 0s to reach the same length. In simple terms, vectorizing makes all reviews uniform, like turning random puzzle pieces into identical blocks so the model can process them consistently.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f92e6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data))\n",
    "\n",
    "print(type(train_data[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc45b34",
   "metadata": {},
   "source": [
    "`print(type(train_data))`\n",
    "\n",
    "**What it does**: This checks the data type of the entire train_data variable.\n",
    "\n",
    "**Why we do it**: The output is `<class 'numpy.ndarray'>`. This confirms that our 25,000 reviews are being held in a NumPy array, which is a special, high-performance kind of list that's standard for machine learning tasks. It's like checking the type of the main container.\n",
    "\n",
    "`print(type(train_data[6]))`\n",
    "\n",
    "**What it does**: This checks the data type of a single item inside the main container. train_data[6] gets the 7th review.\n",
    "\n",
    "**Why we do it**: The output is `<class 'list'>`. This tells us that while the main container is a NumPy array, each individual review inside it is just a regular Python list."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (csci_e89)",
   "language": "python",
   "name": "csci_e89"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
